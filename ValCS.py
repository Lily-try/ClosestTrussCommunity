# This is a sample Python script.
import argparse
import datetime
import os

import networkx as nx
import pandas as pd
import scipy.sparse as sp
import numpy as np
import torch
import torch.nn.functional as F
from matplotlib import pyplot as plt
from scipy.stats import ttest_ind, ttest_rel
from torch_geometric.utils import add_remaining_self_loops

from scipy.sparse import csr_matrix

from utils.citation_loader import citation_graph_reader,citation_target_reader,citation_feature_reader
from models.EmbLearner import EmbLearner
from models.COCLE import COCLE
from models.EmbLearnerWithWeights import EmbLearnerwithWeights
from models.EmbLearnerWithoutHyper import EmbLearnerWithoutHyper
from utils.load_utils import load_data, hypergraph_construction, loadQuerys, load_graph
from utils.log_utils import get_logger, get_log_path
from utils.cocle_val_utils import f1_score_, NMI_score, ARI_score, JAC_score, get_res_path, get_model_path, cal_pre, \
    get_comm_path

import torch, random, itertools, numpy as np
from collections import defaultdict
from scipy.stats import ttest_ind
import matplotlib.pyplot as plt
plt.switch_backend("Agg")
import copy
'''
ä½¿ç”¨å¼•æ–‡ç½‘ç»œç›¸å…³çš„æ•°æ®é›†
è¿™ä¸ªæ˜¯coclepæºç 
'''
def validation(val,nodes_feats, model, edge_index, edge_index_aug):
    scorelists = []
    for q, comm in val:
        h = model((q, None, edge_index, edge_index_aug, nodes_feats))
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        sim=F.cosine_similarity(h[q].unsqueeze(0),h,dim=1) #(115,)
        #ä½¿ç”¨ torch.sigmoid å°†ç›¸ä¼¼åº¦å€¼è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œç„¶åä½¿ç”¨ squeeze(0) ç§»é™¤å¤šä½™çš„ç»´åº¦ï¼Œ
        # å¹¶å°†ç»“æœè½¬ç§»åˆ° CPUï¼Œæœ€åè½¬æ¢ä¸º NumPy æ•°ç»„å¹¶è½¬æ¢ä¸º Python åˆ—è¡¨ã€‚
        simlists = torch.sigmoid(sim.squeeze(0)).to(
            torch.device('cpu')).numpy().tolist()  # torch.sigmoid(simlists).numpy().tolist()
        #å°†ç»“æœå­˜å‚¨åœ¨scorelistsä¸­
        scorelists.append([q, comm, simlists]) #è®°å½•è¯¥æ ·æœ¬çš„æµ‹è¯•ç»“æœ
    s_ = 0.1 #é˜ˆå€¼ï¼Ÿï¼Ÿ
    f1_m = 0.0 #è®°å½•æœ€å¤§çš„æ ·æœ¬å¾—åˆ†
    s_m = s_ #è®°å½•å¯ä»¥å–çš„æœ€å¤§çš„ç¤¾åŒºé˜ˆå€¼
    while(s_<=0.9): #ç»“æŸå¾ªç¯åå¾—åˆ°çš„æ˜¯ä»0.1æŒ‰ç…§0.05çš„æ­¥é•¿ä¸æ–­å¢åŠ ç¤¾åŒºé˜ˆå€¼å¯ä»¥å¾—åˆ°çš„æœ€å¤§çš„å¹³å‡f1å€¼f1_må’Œæœ€ä¼˜çš„s_å–å€¼s_mã€‚
        f1_x = 0.0
        # print("------------------------------", s_) #s_æ˜¯ä»€ä¹ˆï¼Ÿï¼Ÿ
        for q, comm, simlists in scorelists:
            comm_find = []
            for i, score in enumerate(simlists):#iæ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„ç¼–å·ï¼›scoreæ˜¯qä¸æ¯ä¸ªèŠ‚ç‚¹çš„ç›¸ä¼¼å¾—åˆ†ã€‚
                if score >=s_ and i not in comm_find:
                    comm_find.append(i)

            comm_find = set(comm_find)
            comm_find = list(comm_find)
            comm = set(comm)
            comm = list(comm)
            f1, pre, rec = f1_score_(comm_find, comm)
            f1_x= f1_x+f1 #ç´¯åŠ æ­¤æ ·æœ¬çš„f1å¾—åˆ†
        f1_x = f1_x/len(val) #æ€»çš„f1å¾—åˆ†é™¤ä»¥éªŒè¯é›†æ ·æœ¬æ•°é‡
        if f1_m<f1_x: #å¦‚æœæ­¤ç¤¾åŒºé˜ˆå€¼ä¸‹å¾—åˆ°çš„å¹³å‡f1å¾—åˆ†æ›´é«˜
            f1_m = f1_x
            s_m = s_
        s_ = s_+0.05 #å°†s_è¿›è¡Œå¢å¤§ã€‚
    logger.info(f'best threshold: {s_m}, validation_set Avg F1: {f1_m}')
    return s_m, f1_m

def validation_pre(val,nodes_feats, model, edge_index, edge_index_aug):
    '''
    æ”¹ä¸ºé€‰æ‹©precisionæœ€ä¼˜çš„ç»“æœ
    :param val:
    :param nodes_feats:
    :param model:
    :param edge_index:
    :param edge_index_aug:
    :return:
    '''
    scorelists = []
    for q, comm in val:
        h = model((q, None, edge_index, edge_index_aug, nodes_feats))
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        sim = F.cosine_similarity(h[q].unsqueeze(0), h, dim=1)  # (115,)
        # ä½¿ç”¨ torch.sigmoid å°†ç›¸ä¼¼åº¦å€¼è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œç„¶åä½¿ç”¨ squeeze(0) ç§»é™¤å¤šä½™çš„ç»´åº¦ï¼Œ
        # å¹¶å°†ç»“æœè½¬ç§»åˆ° CPUï¼Œæœ€åè½¬æ¢ä¸º NumPy æ•°ç»„å¹¶è½¬æ¢ä¸º Python åˆ—è¡¨ã€‚
        simlists = torch.sigmoid(sim.squeeze(0)).to(
            torch.device('cpu')).numpy().tolist()  # torch.sigmoid(simlists).numpy().tolist()
        # å°†ç»“æœå­˜å‚¨åœ¨scorelistsä¸­
        scorelists.append([q, comm, simlists])  # è®°å½•è¯¥æ ·æœ¬çš„æµ‹è¯•ç»“æœ
    s_ = 0.1  # é˜ˆå€¼ï¼Ÿï¼Ÿ
    pre_m = 0.0
    s_m = s_  # è®°å½•å¯ä»¥å–çš„æœ€å¤§çš„ç¤¾åŒºé˜ˆå€¼
    while (s_ <= 0.9):  # ç»“æŸå¾ªç¯åå¾—åˆ°çš„æ˜¯ä»0.1æŒ‰ç…§0.05çš„æ­¥é•¿ä¸æ–­å¢åŠ ç¤¾åŒºé˜ˆå€¼å¯ä»¥å¾—åˆ°çš„æœ€å¤§çš„å¹³å‡f1å€¼f1_må’Œæœ€ä¼˜çš„s_å–å€¼s_mã€‚
        pre_x = 0.0
        # print("------------------------------", s_) #s_æ˜¯ä»€ä¹ˆï¼Ÿï¼Ÿ
        for q, comm, simlists in scorelists:
            comm_find = []
            for i, score in enumerate(simlists):  # iæ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„ç¼–å·ï¼›scoreæ˜¯qä¸æ¯ä¸ªèŠ‚ç‚¹çš„ç›¸ä¼¼å¾—åˆ†ã€‚
                if score >= s_ and i not in comm_find:
                    comm_find.append(i)

            comm_find = set(comm_find)
            comm_find = list(comm_find)
            comm = set(comm)
            comm = list(comm)
            f1, pre, rec = f1_score_(comm_find, comm)
            pre_x = pre_x + pre  # ç´¯åŠ æ­¤æ ·æœ¬çš„f1å¾—åˆ†
        pre_x = pre_x / len(val)  # æ€»çš„f1å¾—åˆ†é™¤ä»¥éªŒè¯é›†æ ·æœ¬æ•°é‡
        if pre_m < pre_x:  # å¦‚æœæ­¤ç¤¾åŒºé˜ˆå€¼ä¸‹å¾—åˆ°çš„å¹³å‡f1å¾—åˆ†æ›´é«˜
            pre_m = pre_x
            s_m = s_
        s_ = s_ + 0.05  # å°†s_è¿›è¡Œå¢å¤§ã€‚
    logger.info(f'best threshold: {s_m}, validation_set Avg Pre: {pre_m}')
    return s_m, pre_m

def load_citations(args):
    '''********************1. åŠ è½½å›¾æ•°æ®******************************'''
    graphx,n_nodes = load_graph(args.root,args.dataset,args.attack,args.ptb_rate)

    # calAA_start = datetime.datetime.now()
    # # è®¡ç®—aaæŒ‡æ ‡
    # aa_indices = nx.adamic_adar_index(graphx)
    # # åˆå§‹åŒ– Adamic-Adar çŸ©é˜µ
    # aa_matrix = np.zeros((n_nodes, n_nodes))
    # # è®¡ç®— Adamic-Adar æŒ‡æ•°
    # for u, v, p in aa_indices:
    #     aa_matrix[u, v] = p
    #     aa_matrix[v, u] = p  # å› ä¸ºæ˜¯æ— å‘å›¾ï¼Œæ‰€ä»¥ä¹Ÿéœ€è¦å¡«å……å¯¹ç§°ä½ç½®
    # # è½¬æ¢ä¸ºå¼ é‡
    # aa_tensor = torch.tensor(aa_matrix, dtype=torch.float32)
    # logger.info(f'calAA_time = {datetime.datetime.now() - calAA_start}')

    src = []
    dst = []
    for id1, id2 in graphx.edges:
        src.append(id1)
        dst.append(id2)
        src.append(id2)
        dst.append(id1)
    # è¿™ä¸¤è¡Œæ˜¯è·å¾—å­˜å‚¨æˆç¨€ç–çŸ©é˜µçš„æ ¼å¼ï¼ŒåŠ æƒæ¨¡å‹ä¸­ä½¿ç”¨
    num_nodes = graphx.number_of_nodes()
    adj_matrix = csr_matrix(([1] * len(src), (src, dst)), shape=(num_nodes, num_nodes))
    # æ„å»ºè¶…å›¾
    calhyper_start = datetime.datetime.now()
    edge_index = torch.tensor([src, dst])
    edge_index_aug, egde_attr = hypergraph_construction(edge_index, n_nodes, k=args.k)  # æ„å»ºè¶…å›¾
    edge_index = add_remaining_self_loops(edge_index, num_nodes=n_nodes)[0]
    logger.info(f'Cal Hyper_time = {datetime.datetime.now() - calhyper_start}')
    '''2:************************åŠ è½½è®­ç»ƒæ•°æ®**************************'''
    if args.dataset.startswith('stb_'):
        dataset = args.dataset[4:]
    else:
        dataset = args.dataset
    logger.info('æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®')
    train, val, test = loadQuerys(dataset, args.root, args.train_size, args.val_size, args.test_size,
                                  args.train_path, args.test_path, args.val_path)
    logger.info('åŠ è½½è®­ç»ƒæ•°æ®å®Œæˆ')
    '3.*************åŠ è½½ç‰¹å¾æ•°æ®************'
    logger.info('æ­£åœ¨åŠ è½½ç‰¹å¾æ•°æ®')
    if args.dataset in ['cora','pubmed','citeseer']:
        nodes_feats = citation_feature_reader(args.root, dataset)  # numpy.ndaaray:(2708,1433)
        nodes_feats = torch.from_numpy(nodes_feats)  # è½¬æ¢æˆtensor
        node_in_dim = nodes_feats.shape[1]
        print(f'{args.dataset}çš„feats dtype: {nodes_feats.dtype}')
    elif args.dataset in ['cora_stb','cora_gsr','citeseer_stb','citeseer_gsr']:
        nodes_feats = citation_feature_reader(args.root, dataset[:-4])  # numpy.ndaaray:(2708,1433)
        nodes_feats = torch.from_numpy(nodes_feats)  # è½¬æ¢æˆtensor
        node_in_dim = nodes_feats.shape[1]
    elif args.dataset in ['fb107_gsr','fb107_stb']:
        feats_array = np.loadtxt(f'{args.root}/{args.dataset[:-4]}/{args.dataset[:-4]}.feat', delimiter=' ', dtype=np.float32)
        print(type(feats_array))
        # nodes_feats = fnormalize(feats_array)  # å°†ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–
        nodes_feats = torch.from_numpy(feats_array)
        node_in_dim = nodes_feats.shape[1]
    elif args.dataset in ['cocs','photo']:
        with open(f'{args.root}/{args.dataset}/{dataset}.feats', "r") as f:
            # æ¯è¡Œç‰¹å¾è½¬æ¢ä¸ºåˆ—è¡¨ï¼Œç„¶åå †å ä¸º ndarray,æ³¨æ„è¦æ˜¯float32
            nodes_feats = np.array([list(map(float, line.strip().split())) for line in f],dtype=np.float32)
            print(f'{args.dataset}çš„nodes_feats.dtype = {nodes_feats.dtype}')
            print(f'{args.dataset}çš„èŠ‚ç‚¹ç‰¹å¾shape:', nodes_feats.shape)
            nodes_feats = torch.from_numpy(nodes_feats)  # è½¬æ¢æˆtensor
            node_in_dim = nodes_feats.shape[1]
    elif args.dataset in ['fb107','wfb107']:  # ä¸åŠ å…¥ä¸­å¿ƒèŠ‚ç‚¹
        feats_array = np.loadtxt(f'{args.root}/{args.dataset}/{args.dataset}.feat', delimiter=' ', dtype=np.float32)
        print(type(feats_array))
        # nodes_feats = fnormalize(feats_array)  # å°†ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–
        nodes_feats = torch.from_numpy(feats_array)
        node_in_dim = nodes_feats.shape[1]
    elif args.dataset in ['facebook']:  # è¯»å–pygä¸­çš„ç‰¹å¾æ•°æ®
        feats_array = np.loadtxt(f'{args.root}/{args.dataset}/{args.dataset}.feat', dtype=float, delimiter=' ')
        nodes_feats = torch.tensor(feats_array, dtype=torch.float32)
        node_in_dim = nodes_feats.shape[1]
    elif args.dataset in ['football']:
        path_feat = args.root + args.dataset + '/' + args.feats_path
        if not os.path.isfile(path_feat):
            raise Exception("No such file: %s" % path_feat)
        feats_node = {}
        count = 1
        for line in open(path_feat, encoding='utf-8'):
            if count == 1:
                node_n_, node_in_dim = line.split()
                node_in_dim = int(node_in_dim)
                count = count + 1
            else:
                emb = [float(x) for x in line.split()]
                id = int(emb[0])
                emb = emb[1:]
                feats_node[id] = emb
        nodes_feats = []

        for i in range(0, n_nodes):
            if i not in feats_node:
                nodes_feats.append([0.0] * node_in_dim)
            else:
                nodes_feats.append(feats_node[i])
        nodes_feats = torch.tensor(nodes_feats)
    else:
        print('åŠ è½½èŠ‚ç‚¹ç‰¹å¾å¤±è´¥ï¼Œæ•°æ®é›†ä¸åŒ¹é…')
    print('åŠ è½½èŠ‚ç‚¹ç‰¹å¾å®Œæˆå®Œæˆ')
    return nodes_feats, train, val, test, node_in_dim, n_nodes, edge_index, edge_index_aug, adj_matrix  #, aa_tensor

'''ç”¨è¿™ä¸ªåšéªŒè¯'''
def Val_Community_Search(args,logger):

    preprocess_start = datetime.datetime.now()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f'device: {device}')

    #åŠ è½½æ•°æ®å¹¶ç§»åŠ¨åˆ°device
    nodes_feats, train, val, test, node_in_dim, n_nodes, edge_index, edge_index_aug, adj_matrix = load_citations(args)

    # å½’ä¸€åŒ–ä¸€æ¬¡åŸå§‹ç‰¹å¾ï¼Œåé¢ç›´æ¥ç”¨
    X_norm = F.normalize(nodes_feats, p=2, dim=1).to(device)  # (N, d_x)



    logger.info(f'load_time = {datetime.datetime.now() - preprocess_start}, train len = {len(train)}')
    nodes_feats = nodes_feats.to(device)
    edge_index = edge_index.to(device)
    edge_index_aug = edge_index_aug.to(device)

    #åˆ›å»ºèŠ‚ç‚¹åµŒå…¥å­¦ä¹ æ¨¡å‹
    if args.method == 'EmbLearner':
        embLearner = EmbLearner(node_in_dim, args.hidden_dim, args.num_layers, args.drop_out, args.tau, device,args.alpha, args.lam, args.k)  # COCLEPä¸­çš„æ¨¡å‹

    elif args.method == '':
        embLearner = EmbLearnerWithoutHyper(node_in_dim, args.hidden_dim, args.num_layers, args.drop_out, args.tau,device, args.alpha, args.lam, args.k)  # å»æ‰COCLEPä¸­çš„è¶…å›¾è§†å›¾ï¼Œä½†å¾—åˆ°çš„ç»“æœå¾ˆå·®

    elif args.method == 'COCLE':  #è¿™ä¸ªæ˜¯åˆå§‹æœ€é»˜è®¤çš„ç®—æ³•
        embLearner = COCLE(node_in_dim, args.hidden_dim, args.num_layers, args.drop_out, args.tau, device, args.alpha, args.lam, args.k) #COCLEPä¸­çš„æ¨¡å‹ï¼Œç›®å‰å’ŒEmbLearneræ˜¯ä¸€æ ·çš„

    elif args.method == 'EmbLearnerwithWeights': #å°†è¿™ä¸ªä½œä¸ºæˆ‘çš„
        embLearner = EmbLearnerwithWeights(node_in_dim, args.hidden_dim,args.num_layers,args.drop_out,args.tau,device,args.alpha,args.lam,args.k) #ä¼ å…¥edge_weightå‚æ•°çš„æ¨¡å‹
    else:
        raise ValueError(f'method {args.method} not supported')

    logger.info(f'embLearner: {args.method}')

    emb_optim = torch.optim.Adam(embLearner.parameters(), lr=args.lr,weight_decay=args.weight_decay)
    embLearner.to(device)


    logger.info(f'#################### Starting evaluation######################')
    #åŠ è½½æ¨¡å‹å‚æ•°
    bst_model_path = get_model_path('./results/coclep/res_model/',args)
    #ç›®å‰æ˜¯åŠ è½½å…·æœ‰æœ€ä¼˜preçš„æ¨¡å‹
    if args.val_type == 'pre':
        embLearner.load_state_dict(torch.load(f'{bst_model_path}_pre.pkl'))  # åŠ è½½æ¨¡å‹
    else:
        embLearner.load_state_dict(torch.load(f'{bst_model_path}_f1.pkl'))  # åŠ è½½æ¨¡å‹
    embLearner.eval()

    eval_start = datetime.datetime.now()
    # intra_sum, inter_sum = 0.0, 0.0
    # intra_cnt, inter_cnt = 0, 0
    # all_nodes = torch.arange(n_nodes, device=device)

    # ----------------------- åˆå§‹åŒ–ä¸¤ä¸ªç»Ÿè®¡å™¨ -----------------------
    intra_sum_H = inter_sum_H = 0.0
    intra_cnt_H = inter_cnt_H = 0
    intra_sum_X = inter_sum_X = 0.0
    intra_cnt_X = inter_cnt_X = 0

    intra_sum_H_sig = inter_sum_H_sig = 0.0
    intra_cnt_H_sig = inter_cnt_H_sig = 0
    intra_sum_X_sig = inter_sum_X_sig = 0.0
    intra_cnt_X_sig = inter_cnt_X_sig = 0

    pos_scores_raw, neg_scores_raw = [], []  # ç‚¹ç§¯ / ä½™å¼¦
    pos_scores_sig, neg_scores_sig = [], []  # sigmoid å

    all_nodes = torch.arange(n_nodes, device=device)

    with torch.no_grad():
        #ä½¿ç”¨éªŒè¯é›†æ•°æ®æ‰¾æ‰“æœ€ä½³é˜ˆå€¼s_
        if args.val_type == 'f1':
            s_, f1_ = validation(val, nodes_feats, embLearner, edge_index, edge_index_aug)
            logger.info(f'evaluation time = {datetime.datetime.now() - eval_start}, best s_={s_}, best val f1_={f1_}')
        elif args.val_type == 'pre':
            s_, pre_ = validation_pre(val, nodes_feats, embLearner, edge_index, edge_index_aug)
            logger.info(f'evaluation time = {datetime.datetime.now() - eval_start}, best s_={s_}, best val pre_={pre_}')
        val_running_time = (datetime.datetime.now() - eval_start).seconds  # ç»“æŸäº†æµ‹è¯•è¿è¡Œçš„æ—¶é—´
        logger.info(f'éªŒè¯ç»“æŸï¼Œç”¨æ—¶ï¼šval_running_time')
        logger.info(f'#################### starting test  ####################')
        for q, comm in test:
            h = embLearner((q, None, edge_index, edge_index_aug, nodes_feats))
            # h = F.normalize(h, p=2, dim=1)

            comm_idx = torch.tensor(comm, device=h.device, dtype=torch.long)
            out_idx = all_nodes[~torch.isin(all_nodes, comm_idx)]

            # ---- (1) ç¤¾åŒºå†…ä¸¤ä¸¤ç›¸ä¼¼ ----
            if len(comm_idx) > 1:
                h_c = h[comm_idx]  # (m,d)
                h_c = F.normalize(h_c, p=2, dim=1) #
                sims = torch.mm(h_c, h_c.T)
                sims_sig = torch.sigmoid(sims)  # â˜… æ–°å¢ sigmoid æ˜ å°„

                iu = torch.triu_indices(len(comm_idx), len(comm_idx), offset=1)
                intra_sum_H += sims[iu[0], iu[1]].sum().item()
                intra_cnt_H += iu.size(1)

                intra_sum_H_sig += sims_sig[iu[0], iu[1]].sum().item()
                intra_cnt_H_sig += iu.size(1)

            # ---- (2) ç¤¾åŒºâ†”å¤–éƒ¨ ----
            h_out = h[out_idx]  # (n,d)
            h_out = F.normalize(h_out, p=2, dim=1)
            h_c = h[comm_idx]
            h_c = F.normalize(h_c, p=2, dim=1)
            sims2 = torch.mm(h_c, h_out.T)  # (m,n)
            sims2_sig = torch.sigmoid(sims2)  # â˜… æ–°å¢ sigmoid æ˜ å°„

            inter_sum_H += sims2.sum().item()
            inter_cnt_H += sims2.numel()

            inter_sum_H_sig += sims2_sig.sum().item()
            inter_cnt_H_sig += sims2_sig.numel()

            # ---------- 2) åŸå§‹ç‰¹å¾ X ----------
            X_c = X_norm[comm_idx]  # (m, d_x)
            X_out = X_norm[out_idx]  # (n, d_x)

            if len(comm_idx) > 1:
                sims_x = torch.mm(X_c, X_c.T)
                sims_x_sig = torch.sigmoid(sims_x)
                iu_x = torch.triu_indices(len(comm_idx), len(comm_idx), offset=1)
                intra_sum_X += sims_x[iu_x[0], iu_x[1]].sum().item()
                intra_cnt_X += iu_x.size(1)
                intra_sum_X_sig += sims_x_sig[iu_x[0], iu_x[1]].sum().item()
                intra_cnt_X_sig += iu_x.size(1)

            sims2_x = torch.mm(X_c, X_out.T)
            sims2_x_sig = torch.sigmoid(sims2_x)
            inter_sum_X += sims2_x.sum().item()
            inter_cnt_X += sims2_x.numel()

            inter_sum_X_sig += sims2_x_sig.sum().item()
            inter_cnt_X_sig += sims2_x_sig.numel()
        # ----------------------- è®¡ç®—å¹³å‡å€¼ -----------------------
        Î¼_intra_H = intra_sum_H / intra_cnt_H
        Î¼_inter_H = inter_sum_H / inter_cnt_H
        Î¼_intra_X = intra_sum_X / intra_cnt_X
        Î¼_inter_X = inter_sum_X / inter_cnt_X

        Î¼_intra_H_sig = intra_sum_H_sig / intra_cnt_H_sig
        Î¼_inter_H_sig = inter_sum_H_sig / inter_cnt_H_sig
        Î¼_intra_X_sig = intra_sum_X_sig / intra_cnt_X_sig
        Î¼_inter_X_sig = inter_sum_X_sig / inter_cnt_X_sig

        logger.info(f"H:  Î¼_intra={Î¼_intra_H:.4f}, Î¼_inter={Î¼_inter_H:.4f}")
        logger.info(f"H(sigmoid):  Î¼_intra={Î¼_intra_H_sig:.4f}, Î¼_inter={Î¼_inter_H_sig:.4f}")
        logger.info(f"X:  Î¼_intra={Î¼_intra_X:.4f}, Î¼_inter={Î¼_inter_X:.4f}")
        logger.info(f"X(sigmoid):  Î¼_intra={Î¼_intra_X_sig:.4f}, Î¼_inter={Î¼_inter_X_sig:.4f}")

        return Î¼_intra_H, Î¼_inter_H, Î¼_intra_X, Î¼_inter_X


def Val_Community_Search_zhifang(args,logger):

    preprocess_start = datetime.datetime.now()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f'device: {device}')

    #åŠ è½½æ•°æ®å¹¶ç§»åŠ¨åˆ°device
    nodes_feats, train, val, test, node_in_dim, n_nodes, edge_index, edge_index_aug, adj_matrix = load_citations(args)

    # å½’ä¸€åŒ–ä¸€æ¬¡åŸå§‹ç‰¹å¾ï¼Œåé¢ç›´æ¥ç”¨
    X_norm = F.normalize(nodes_feats, p=2, dim=1).to(device)  # (N, d_x)



    logger.info(f'load_time = {datetime.datetime.now() - preprocess_start}, train len = {len(train)}')
    nodes_feats = nodes_feats.to(device)
    edge_index = edge_index.to(device)
    edge_index_aug = edge_index_aug.to(device)

    #åˆ›å»ºèŠ‚ç‚¹åµŒå…¥å­¦ä¹ æ¨¡å‹
    if args.method == 'EmbLearner':
        embLearner = EmbLearner(node_in_dim, args.hidden_dim, args.num_layers, args.drop_out, args.tau, device,args.alpha, args.lam, args.k)  # COCLEPä¸­çš„æ¨¡å‹

    elif args.method == '':
        embLearner = EmbLearnerWithoutHyper(node_in_dim, args.hidden_dim, args.num_layers, args.drop_out, args.tau,device, args.alpha, args.lam, args.k)  # å»æ‰COCLEPä¸­çš„è¶…å›¾è§†å›¾ï¼Œä½†å¾—åˆ°çš„ç»“æœå¾ˆå·®

    elif args.method == 'COCLE':  #è¿™ä¸ªæ˜¯åˆå§‹æœ€é»˜è®¤çš„ç®—æ³•
        embLearner = COCLE(node_in_dim, args.hidden_dim, args.num_layers, args.drop_out, args.tau, device, args.alpha, args.lam, args.k) #COCLEPä¸­çš„æ¨¡å‹ï¼Œç›®å‰å’ŒEmbLearneræ˜¯ä¸€æ ·çš„

    elif args.method == 'EmbLearnerwithWeights': #å°†è¿™ä¸ªä½œä¸ºæˆ‘çš„
        embLearner = EmbLearnerwithWeights(node_in_dim, args.hidden_dim,args.num_layers,args.drop_out,args.tau,device,args.alpha,args.lam,args.k) #ä¼ å…¥edge_weightå‚æ•°çš„æ¨¡å‹
    else:
        raise ValueError(f'method {args.method} not supported')

    logger.info(f'embLearner: {args.method}')

    emb_optim = torch.optim.Adam(embLearner.parameters(), lr=args.lr,weight_decay=args.weight_decay)
    embLearner.to(device)


    logger.info(f'#################### Starting evaluation######################')
    #åŠ è½½æ¨¡å‹å‚æ•°
    bst_model_path = get_model_path('./results/coclep/res_model/',args)
    #ç›®å‰æ˜¯åŠ è½½å…·æœ‰æœ€ä¼˜preçš„æ¨¡å‹
    if args.val_type == 'pre':
        embLearner.load_state_dict(torch.load(f'{bst_model_path}_pre.pkl'))  # åŠ è½½æ¨¡å‹
    else:
        embLearner.load_state_dict(torch.load(f'{bst_model_path}_f1.pkl'))  # åŠ è½½æ¨¡å‹
    embLearner.eval()

    eval_start = datetime.datetime.now()
    # intra_sum, inter_sum = 0.0, 0.0
    # intra_cnt, inter_cnt = 0, 0
    # all_nodes = torch.arange(n_nodes, device=device)

    # ----------------------- åˆå§‹åŒ–ä¸¤ä¸ªç»Ÿè®¡å™¨ -----------------------
    intra_sum_H = inter_sum_H = 0.0
    intra_cnt_H = inter_cnt_H = 0
    intra_sum_X = inter_sum_X = 0.0
    intra_cnt_X = inter_cnt_X = 0

    intra_sum_H_sig = inter_sum_H_sig = 0.0
    intra_cnt_H_sig = inter_cnt_H_sig = 0
    intra_sum_X_sig = inter_sum_X_sig = 0.0
    intra_cnt_X_sig = inter_cnt_X_sig = 0

    pos_scores_raw, neg_scores_raw = [], []  # ç‚¹ç§¯ / ä½™å¼¦
    pos_scores_sig, neg_scores_sig = [], []  # sigmoid å

    all_nodes = torch.arange(n_nodes, device=device)

    with torch.no_grad():
        #ä½¿ç”¨éªŒè¯é›†æ•°æ®æ‰¾æ‰“æœ€ä½³é˜ˆå€¼s_
        if args.val_type == 'f1':
            s_, f1_ = validation(val, nodes_feats, embLearner, edge_index, edge_index_aug)
            logger.info(f'evaluation time = {datetime.datetime.now() - eval_start}, best s_={s_}, best val f1_={f1_}')
        elif args.val_type == 'pre':
            s_, pre_ = validation_pre(val, nodes_feats, embLearner, edge_index, edge_index_aug)
            logger.info(f'evaluation time = {datetime.datetime.now() - eval_start}, best s_={s_}, best val pre_={pre_}')
        val_running_time = (datetime.datetime.now() - eval_start).seconds  # ç»“æŸäº†æµ‹è¯•è¿è¡Œçš„æ—¶é—´
        logger.info(f'éªŒè¯ç»“æŸï¼Œç”¨æ—¶ï¼šval_running_time')
        logger.info(f'#################### starting test  ####################')
        for q, comm in test:
            h = embLearner((q, None, edge_index, edge_index_aug, nodes_feats))
            # h = F.normalize(h, p=2, dim=1) #æ–¹ä¾¿ç›´æ¥ç”¨ç‚¹ç§¯=ä½™å¼¦
            comm_idx = torch.tensor(comm, device=h.device, dtype=torch.long)
            out_idx = all_nodes[~torch.isin(all_nodes, comm_idx)]

            # ---- (1) ç¤¾åŒºå†…ä¸¤ä¸¤ç›¸ä¼¼ ----
            if len(comm_idx) > 1:
                h_c = h[comm_idx]  # (m,d)
                h_c = F.normalize(h_c, p=2, dim=1) #
                sims = torch.mm(h_c, h_c.T)
                sims_sig = torch.sigmoid(sims)  # â˜… æ–°å¢ sigmoid æ˜ å°„

                iu = torch.triu_indices(len(comm_idx), len(comm_idx), offset=1)

                # ç”¨äºç”»ç›´æ–¹å›¾
                pos_raw = sims[iu[0], iu[1]]  # (mÂ·(m-1)/2, )
                pos_scores_raw.extend(pos_raw.tolist())
                pos_scores_sig.extend(torch.sigmoid(pos_raw).tolist())

                intra_sum_H += sims[iu[0], iu[1]].sum().item()
                intra_cnt_H += iu.size(1)

                intra_sum_H_sig += sims_sig[iu[0], iu[1]].sum().item()
                intra_cnt_H_sig += iu.size(1)

            # ---- (2) ç¤¾åŒºâ†”å¤–éƒ¨ ----
            h_out = h[out_idx]  # (n,d)
            h_out = F.normalize(h_out, p=2, dim=1)
            h_c = h[comm_idx]
            h_c = F.normalize(h_c, p=2, dim=1)
            sims2 = torch.mm(h_c, h_out.T)  # (m,n)
            #ç”¨äºç”»ç›´æ–¹å›¾
            neg_raw = sims2.flatten()
            neg_scores_raw.extend(neg_raw.tolist())
            neg_scores_sig.extend(torch.sigmoid(neg_raw).tolist())

            sims2_sig = torch.sigmoid(sims2)  # â˜… æ–°å¢ sigmoid æ˜ å°„

            inter_sum_H += sims2.sum().item()
            inter_cnt_H += sims2.numel()

            inter_sum_H_sig += sims2_sig.sum().item()
            inter_cnt_H_sig += sims2_sig.numel()

            # ---------- 2) åŸå§‹ç‰¹å¾ X ----------
            X_c = X_norm[comm_idx]  # (m, d_x)
            X_out = X_norm[out_idx]  # (n, d_x)

            if len(comm_idx) > 1:
                sims_x = torch.mm(X_c, X_c.T)
                sims_x_sig = torch.sigmoid(sims_x)
                iu_x = torch.triu_indices(len(comm_idx), len(comm_idx), offset=1)
                intra_sum_X += sims_x[iu_x[0], iu_x[1]].sum().item()
                intra_cnt_X += iu_x.size(1)
                intra_sum_X_sig += sims_x_sig[iu_x[0], iu_x[1]].sum().item()
                intra_cnt_X_sig += iu_x.size(1)

            sims2_x = torch.mm(X_c, X_out.T)
            sims2_x_sig = torch.sigmoid(sims2_x)
            inter_sum_X += sims2_x.sum().item()
            inter_cnt_X += sims2_x.numel()

            inter_sum_X_sig += sims2_x_sig.sum().item()
            inter_cnt_X_sig += sims2_x_sig.numel()
        # ----------------------- è®¡ç®—å¹³å‡å€¼ -----------------------
        Î¼_intra_H = intra_sum_H / intra_cnt_H
        Î¼_inter_H = inter_sum_H / inter_cnt_H
        Î¼_intra_X = intra_sum_X / intra_cnt_X
        Î¼_inter_X = inter_sum_X / inter_cnt_X

        Î¼_intra_H_sig = intra_sum_H_sig / intra_cnt_H_sig
        Î¼_inter_H_sig = inter_sum_H_sig / inter_cnt_H_sig
        Î¼_intra_X_sig = intra_sum_X_sig / intra_cnt_X_sig
        Î¼_inter_X_sig = inter_sum_X_sig / inter_cnt_X_sig

        logger.info(f"H:  Î¼_intra={Î¼_intra_H:.4f}, Î¼_inter={Î¼_inter_H:.4f}")
        logger.info(f"H(sigmoid):  Î¼_intra={Î¼_intra_H_sig:.4f}, Î¼_inter={Î¼_inter_H_sig:.4f}")
        logger.info(f"X:  Î¼_intra={Î¼_intra_X:.4f}, Î¼_inter={Î¼_inter_X:.4f}")
        logger.info(f"X(sigmoid):  Î¼_intra={Î¼_intra_X_sig:.4f}, Î¼_inter={Î¼_inter_X_sig:.4f}")

        # -------- (3) ç»˜å›¾(ç¤ºä¾‹) --------
        print('å¼€å§‹ç»˜å›¾')

        # ---------- 1) ä¿å­˜åŸå§‹æ‰“åˆ†ï¼ˆ4 ç»„ï¼‰ ----------
        os.makedirs("Visual/tongji", exist_ok=True)

        np.savetxt("Visual/tongji/pos_scores_raw.txt", np.array(pos_scores_raw), fmt="%.6f")
        np.savetxt("Visual/tongji/neg_scores_raw.txt", np.array(neg_scores_raw), fmt="%.6f")
        np.savetxt("Visual/tongji/pos_scores_sig.txt", np.array(pos_scores_sig), fmt="%.6f")
        np.savetxt("Visual/tongji/neg_scores_sig.txt", np.array(neg_scores_sig), fmt="%.6f")

        logger.info("âœ… å·²å¯¼å‡ºåŸå§‹åˆ†æ•°å­—ç¬¦ä¸²åˆ° Visual/tongji/*.txt")

        # ---------- 2) å¦‚éœ€æå‰ç®—å¥½ç›´æ–¹å›¾ ----------
        # è¿™é‡Œç”¨ 50 ä¸ª binï¼ˆ-1~1ï¼‰ï¼Œä½ å¯æŒ‰éœ€ä¿®æ”¹
        bins = np.linspace(-1, 1, 51)  # 50 bins => 51 ä¸ªåˆ†å‰²ç‚¹
        centers = 0.5 * (bins[:-1] + bins[1:])  # bin ä¸­å¿ƒ

        hist_pos_raw, _ = np.histogram(pos_scores_raw, bins=bins)
        hist_neg_raw, _ = np.histogram(neg_scores_raw, bins=bins)

        hist_pos_sig, _ = np.histogram(pos_scores_sig, bins=np.linspace(0, 1, 51))
        hist_neg_sig, _ = np.histogram(neg_scores_sig, bins=np.linspace(0, 1, 51))

        # ä¿å­˜ raw ç›´æ–¹å›¾
        df_raw = pd.DataFrame({
            "bin_center": centers,
            "pos_count": hist_pos_raw,
            "neg_count": hist_neg_raw
        })
        df_raw.to_csv("Visual/tongji/hist_raw.csv", index=False)

        # ä¿å­˜ sigmoid ç›´æ–¹å›¾ï¼ˆæ³¨æ„ä¸­å¿ƒ 0~1ï¼‰
        df_sig = pd.DataFrame({
            "bin_center": 0.5 * (np.linspace(0, 1, 51)[:-1] + np.linspace(0, 1, 51)[1:]),
            "pos_count": hist_pos_sig,
            "neg_count": hist_neg_sig
        })
        df_sig.to_csv("Visual/tongji/hist_sigmoid.csv", index=False)

        logger.info("âœ… å·²å¯¼å‡ºç›´æ–¹å›¾è®¡æ•°åˆ° export/hist_*.csv")

        # plot_histogram_save(
        #     pos_scores_raw, neg_scores_raw,
        #     title="Raw cosine similarity distribution",
        #     xlabel="cosine similarity", bins=50,
        #     save_path="Visual/tongji/raw_sim_hist.png"  # ğŸ‘ˆ æŒ‡å®šæ–‡ä»¶å
        # )
        #
        # plot_histogram_save(
        #     pos_scores_sig, neg_scores_sig,
        #     title="Sigmoid-mapped similarity distribution",
        #     xlabel="sigmoid(sim)", bins=50,
        #     save_path="Visual/tongji/sigmoid_sim_hist.png"
        # )
        # # plot_histogram(
        #     pos_scores_raw, neg_scores_raw,
        #     title="Raw cosine similarity distribution",
        #     xlabel="cosine similarity", bins=50
        # )
        # plot_histogram(
        #     pos_scores_sig, neg_scores_sig,
        #     title="Sigmoid-mapped similarity distribution",
        #     xlabel="sigmoid(sim)", bins=50
        # )
        return Î¼_intra_H, Î¼_inter_H, Î¼_intra_X, Î¼_inter_X


def plot_histogram_save(pos, neg, title, xlabel, bins=50, save_path=None):
    plt.figure(figsize=(6, 4))
    plt.hist(pos, bins=bins, alpha=0.6, label="intra (positive)", density=True)
    plt.hist(neg, bins=bins, alpha=0.6, label="inter (negative)", density=True)
    plt.xlabel(xlabel)
    plt.ylabel("density")
    plt.title(title)
    plt.legend()
    plt.tight_layout()

    if save_path is None:          # å¦‚æœæ²¡ç»™è·¯å¾„å°±æ‹¼ä¸€ä¸ª
        safe_title = title.lower().replace(" ", "_")
        save_path = f"{safe_title}.png"

    os.makedirs(os.path.dirname(save_path) or ".", exist_ok=True)
    plt.savefig(save_path, dpi=300)
    plt.close()                    # é‡Šæ”¾å†…å­˜ / ä¸é˜»å¡


def plot_histogram(pos, neg, title, xlabel, bins=50):
    plt.figure(figsize=(6, 4))
    plt.hist(pos, bins=bins, alpha=0.6, label="intra (positive)", density=True)
    plt.hist(neg, bins=bins, alpha=0.6, label="inter (negative)", density=True)
    plt.xlabel(xlabel)
    plt.ylabel("density")
    plt.title(title)
    plt.legend()
    plt.tight_layout()
    plt.show()

# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    # é‡å¤æ¬¡æ•°ï¼Œè®ºæ–‡ä¸­å¸¸ç”¨çš„é‡å¤5æ¬¡å–å¹³å‡
    parser.add_argument('--count', type=int, default=1)
    parser.add_argument('--root', type=str, default='./data')
    # parser.add_argument('--res_root', type=str, default='./results/', help='result path')
    # parser.add_argument("--log", action='store_true', help='run prepare_data or not')
    parser.add_argument("--log", type=bool,default=True, help='run prepare_data or not')
    # è®­ç»ƒå®Œæ¯•çš„æ¨¡å‹çš„å­˜å‚¨è·¯å¾„
    parser.add_argument('--method',type=str,default='COCLE',choices=['EmbLearner','COCLE','EmbLearnerWithoutHyper','EmbLearnerwithWeights'])
    parser.add_argument('--model_path', type=str, default='CS')
    parser.add_argument('--m_model_path', type=str, default='META')

    # æ•°æ®é›†é€‰é¡¹
    parser.add_argument('--dataset', type=str, default='cora')
    # è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†å¤§å°ï¼Œä»¥åŠç›¸åº”çš„æ–‡ä»¶è·¯å¾„ï¼ŒèŠ‚ç‚¹ç‰¹å¾å­˜å‚¨è·¯å¾„
    parser.add_argument('--train_size', type=int, default=300)
    parser.add_argument('--val_size', type=int, default=100)
    parser.add_argument('--test_size', type=int, default=500)
    parser.add_argument('--train_path', type=str, default='3_pos_train')
    parser.add_argument('--test_path', type=str, default='3_test')
    parser.add_argument('--val_path', type=str, default='3_val')
    parser.add_argument('--feats_path', type=str, default='feats.txt')
    parser.add_argument('--val_type', type=str, default='f1',help='pre or f1 to val')
    # æ§åˆ¶æ”»å‡»æ–¹æ³•ã€æ”»å‡»ç±»å‹å’Œæ”»å‡»ç‡
    #choices=['none','meta', 'random_remove','random_flip','random_add', 'meta_attack','add','del','gflipm','gdelm','gaddm','cdelm','cflipm','delm','flipm']
    parser.add_argument('--attack', type=str, default='none')
    parser.add_argument('--type', type=str, default='add', help='random attack type', choices=['add', 'remove', 'flip'])
    parser.add_argument('--noise_level', type=int, default=3, choices=[1, 2, 3], help='noisy level')
    parser.add_argument('--ptb_rate', type=float, default=0.30, help='pertubation rate')

    # æ¨¡å‹batchå¤§å°ï¼Œéšè—å±‚ç»´åº¦ï¼Œè®­ç»ƒepochæ•°ï¼Œdrop_outï¼Œå­¦é¡»ç‡lrï¼Œæƒé‡è¡°å‡weight_decay
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--hidden_dim', type=int, default=256)
    parser.add_argument('--num_layers', type=int, default=3)
    parser.add_argument('--epoch_n', type=int, default=10)
    parser.add_argument('--drop_out', type=float, default=0.1)
    parser.add_argument('--lr', type=float, default=0.001)  # åŸæ–‡é»˜è®¤çš„æ˜¯0.001ï¼Œè°ƒæ•´å¤§ä¸€äº›0.1ã€‚
    parser.add_argument('--weight_decay', type=float, default=0.0005)
    # æ³¨æ„åŠ›ç³»æ•°tauï¼Œä¸åŒæŸå¤±å‡½çš„æ¯”ç‡ï¼Œè¶…å›¾è·³æ•°k
    parser.add_argument('--tau', type=float, default=0.2)
    parser.add_argument('--alpha', type=float, default=0.2)
    parser.add_argument('--lam', type=float, default=0.2)
    # è¶…å›¾çš„è·³æ•°ï¼Œè®ºæ–‡ä¸­ä½¿ç”¨çš„æ˜¯1
    parser.add_argument('--k', type=int, default=2)

    # æƒé‡è®¡ç®—æ¨¡å‹åŠå…¶å­¦ä¹ ç‡
    parser.add_argument('--mw_net', type=str, default='MLP', choices=['MLP', 'GCN'], help='type of meta-weighted model')
    parser.add_argument('--m_lr', type=float, default=0.005, help='learning rate of meta model')

    # æ›´æ–°å›¾çš„é˜ˆå€¼ã€‚paåŠ è¾¹é˜ˆå€¼ï¼Œpdåˆ è¾¹é˜ˆå€¼ã€‚
    parser.add_argument('--pa', type=float, default=0.7)
    parser.add_argument('--pd', type=float, default=0.3)
    parser.add_argument('--n_p', type=int, default=5, help='number of positive pairs per node')
    parser.add_argument("--n_n", type=int, default=5, help='number of negitive pairs per node')
    parser.add_argument('--sigma', type=float, default=100,
                        help='the parameter to control the variance of sample weights in rec loss')
    parser.add_argument('--t_delete', type=float, default=0.1,
                        help='threshold of eliminating the edges')
    parser.add_argument('--gamma', type=float, default=0.01,
                        help='weight of rec loss')

    args = parser.parse_args()
    if args.log:
        log_path = get_log_path('./log/coclep/', args)
        logger = get_logger(log_path)
        print(f'save logger to {log_path}')
    else:
        logger = get_logger()

    Î¼_intra_H, Î¼_inter_H, Î¼_intra_X, Î¼_inter_X = Val_Community_Search_zhifang(args, logger)
    print(f"{args.dataset}_{args.attack}_{args.ptb_rate}ï¼šÎ¼_intra_H = {Î¼_intra_H:.4f},  Î¼_inter = {Î¼_inter_H:.4f}")
    print(f"{args.dataset}_{args.attack}_{args.ptb_rate}ï¼šÎ¼_intra_x = {Î¼_intra_X:.4f},  Î¼_inter = {Î¼_inter_X:.4f}")
